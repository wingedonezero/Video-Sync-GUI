Of course. Here is a complete and detailed breakdown of the final script we've built, outlining its features, the step-by-step workflow, and the libraries it uses.

High-Level Purpose ðŸŽ¯

The Video Sync TUI is a command-line tool designed to solve a common problem in media collecting: synchronizing high-quality video from one source (like a Japanese Blu-ray) with audio and subtitles from another source (like a US Blu-ray).

It automates the entire process of:

    Finding the precise time difference (delay) between the video files.

    Extracting the desired audio, subtitles, and attachments.

    Merging them into a new, perfectly synchronized video file while preserving all metadata and quality.

It can operate on single files or run in a batch mode to process entire folders of episodes automatically.

Core Features

    Terminal User Interface (TUI): The script provides a user-friendly, keyboard-navigable interface using the curses library, eliminating the need for complex command-line arguments.

    Dual Analysis Modes: You can choose the best method for finding the delay between files:

        Audio Cross-Correlation: A highly accurate method that mathematically compares the audio waveforms of two files to find the precise offset.

        VideoDiff: A very fast alternative that uses "perceptual hashing" to compare video frames, which is excellent for files that are visually almost identical.

    Full "Analyze & Merge" Pipeline: The primary workflow that automates the entire process from analysis to final output file.

    "Analyze Only" Mode: A simpler mode that only calculates and reports the delay, without modifying any files.

    Batch Processing: The script can process entire folders of matching files (e.g., Episode 01.mkv, Episode 02.mkv, etc.) in a single run.

    Advanced Track & Metadata Control:

        Reference Preservation: Keeps all original video, audio, and subtitle tracks from the reference file, only stripping their track names for a cleaner output.

        Tertiary File Support: Can merge in subtitles and attachments (fonts) from a third source file.

        Smart Subtitle Ordering: Correctly orders subtitles from the 3rd file before the 2nd file's subtitles in the final merge.

        Intelligent Default Subtitle Flagging: Automatically sets the "default" flag on subtitle tracks named "Signs," "Songs," or "Titles" for a better viewing experience.

    Chapter Renaming: An optional feature to extract chapters from the reference file, rename them sequentially (e.g., "Chapter 01, Chapter 02"), and replace the old chapters with the new, clean set.

    Persistent Settings: All your global options (output folder, analysis mode, etc.) are saved to a settings.json file and loaded automatically.

    Robust Error Handling & Reporting: The script is designed to handle errors gracefully, provides detailed logs for each job, and presents a clear summary report at the end of a batch.

Workflow in Detail (Analyze & Merge)

Here is the precise, step-by-step process the script follows for each pair of matching files in a job:

Step 1: Analysis
The script determines the time offset for the secondary and (if present) tertiary files.

    If Secondary File is Present:

        It finds the delay between the Reference and Secondary files.

        In Audio Correlation mode, it compares the first audio track of the reference file against the first Japanese (jpn) audio track of the secondary file (unless disabled in settings).

        This calculated delay is stored as delay_sec.

    If Tertiary File is Present:

        It performs a second, independent analysis to find the delay between the Reference and Tertiary files.

        In Audio Correlation mode, it compares the first audio track of the reference against the first Japanese (jpn) audio track of the third file (if one exists), otherwise it uses the first audio track it finds.

        This delay is stored as delay_ter.

Step 2: Consistency Check (Audio Mode Only)
After an audio analysis, the script checks if the results are reliable.

    What: It looks at the standard deviation and average match percentage of the 10 analysis chunks.

    Why: If the delays are wildly different (high standard deviation) or the confidence scores are very low, it indicates a poor match (e.g., different edits of a show). The script will pause and show you the results, asking for confirmation (y/n) before proceeding. This prevents a bad merge.

Step 3: Chapter Renaming (Optional)
If the "Rename Chapters" setting is Yes:

    What: It uses mkvextract to pull the chapters from the Reference file into a temporary XML file. It then uses Python's XML library to rename each chapter sequentially ("Chapter 01," "Chapter 02," etc.) and saves a new, modified XML file.

    Why: This cleans up inconsistent or missing chapter names from the original disc rip, providing a better user experience.

Step 4: Extraction
The script creates a temporary folder and uses mkvextract to pull all the necessary tracks.

    What:

        It extracts all tracks from the Reference file, preserving their metadata.

        It extracts all English audio and all subtitles from the Secondary file.

        It extracts all subtitles and all attachments from the Tertiary file.

    Why: This "pure extraction" method is the most reliable way to handle merging. It creates a set of simple, single-track files, which avoids the complex command-line syntax that was causing errors.

Step 5: Final Merge
This is the most critical step, where the final file is assembled.

    What: The script builds and executes a single, precise mkvmerge command. It adds the extracted track files one-by-one in a specific order, applying all the necessary options to each one individually.

    Why: This method gives us complete control over the final file structure and ensures all metadata is correctly applied. The final track order is:

        Reference Video

        Secondary English Audio (with delay_sec applied)

        Original Reference Audio

        Tertiary Subtitles (with delay_ter applied)

        Secondary Subtitles (with delay_sec applied)

        Original Reference Subtitles and other tracks

        Tertiary Attachments

Step 6: Cleanup

    What: The temporary folder and all of its contents (extracted tracks, modified chapter files) are automatically deleted.

    Why: This ensures the script leaves no garbage behind and your temporary directory remains clean.

Step 7: Reporting

    What: The script displays a final summary screen showing the status (Success/Failed), the calculated delays, and the output file for each job in the batch.

    Why: This gives you a clear, immediate confirmation of what was done.

Libraries Used

    Standard Libraries:

        curses: Provides the foundation for the entire Terminal User Interface (TUI), handling windows, text input, and colors.

        os, sys, subprocess, pathlib: Used for essential operating system interactions like managing files and folders, finding executables, and running external command-line tools (ffmpeg, mkvmerge, etc.).

        tempfile: Used to safely create and automatically clean up the temporary directory for extracted files.

        re: Used for regular expressions, primarily for parsing the output of the videodiff tool.

        logging: Handles the creation of detailed log files for each job.

        time: Used for minor delays in the UI for a smoother user experience.

        locale: Ensures the script can correctly handle non-English characters in file paths.

        json: Used for reading and writing the settings.json configuration file.

        shutil: Used to find the absolute paths of the required command-line tools.

        xml.etree.ElementTree: A built-in library used for the chapter renaming feature.

        collections.Counter: A helper used to efficiently find the most frequent delay in the analysis results.

    External Libraries (must be installed via pip):

        numpy: The fundamental package for scientific computing in Python. It's required by scipy and librosa to handle the audio data as numerical arrays.

        scipy: A library for scientific and technical computing. We use its powerful signal.correlate function to perform the cross-correlation on the audio arrays.

        librosa: A popular library for audio analysis. We use it as a robust and reliable backend to read the audio data from the temporary .wav files into NumPy arrays that SciPy can process.
